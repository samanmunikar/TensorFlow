{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Predicting Stock Price using TensorFlow2 and Keras",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZHUT2mrzzN4",
        "colab_type": "code",
        "outputId": "92510ed0-309d-49a8-d9d8-8d7273f2e3df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 989
        }
      },
      "source": [
        "pip install tensorflow==2.0"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/46/0f/7bd55361168bb32796b360ad15a25de6966c9c1beb58a8e30c01c8279862/tensorflow-2.0.0-cp36-cp36m-manylinux2010_x86_64.whl (86.3MB)\n",
            "\u001b[K     |████████████████████████████████| 86.3MB 74kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (1.17.5)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (1.12.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (1.27.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (0.1.8)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (1.1.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (1.0.8)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (0.2.2)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (3.10.0)\n",
            "Collecting tensorflow-estimator<2.1.0,>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fc/08/8b927337b7019c374719145d1dceba21a8bb909b93b1ad6f8fb7d22c1ca1/tensorflow_estimator-2.0.1-py2.py3-none-any.whl (449kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 54.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (1.11.2)\n",
            "Collecting tensorboard<2.1.0,>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/54/99b9d5d52d5cb732f099baaaf7740403e83fe6b0cedde940fabd2b13d75a/tensorboard-2.0.2-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 36.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (0.9.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (0.34.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (3.1.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (1.1.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (0.8.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==2.0) (2.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==2.0) (45.1.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (1.0.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (2.21.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (0.4.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (1.7.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (3.2.1)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (1.24.3)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (1.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (0.2.8)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (4.0)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (3.1.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (0.4.8)\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, tensorflow\n",
            "  Found existing installation: tensorflow-estimator 1.15.1\n",
            "    Uninstalling tensorflow-estimator-1.15.1:\n",
            "      Successfully uninstalled tensorflow-estimator-1.15.1\n",
            "  Found existing installation: tensorboard 1.15.0\n",
            "    Uninstalling tensorboard-1.15.0:\n",
            "      Successfully uninstalled tensorboard-1.15.0\n",
            "  Found existing installation: tensorflow 1.15.0\n",
            "    Uninstalling tensorflow-1.15.0:\n",
            "      Successfully uninstalled tensorflow-1.15.0\n",
            "Successfully installed tensorboard-2.0.2 tensorflow-2.0.0 tensorflow-estimator-2.0.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tensorboard",
                  "tensorflow",
                  "tensorflow_core",
                  "tensorflow_estimator"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HyDV5Zyz3Zb",
        "colab_type": "code",
        "outputId": "061efb37-2868-4195-df47-e32246066a7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow \n",
        "print(tensorflow.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aae6RedUz6C9",
        "colab_type": "code",
        "outputId": "e834e703-5214-4390-88a1-07e97a0738ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "pip install yahoo_fin"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting yahoo_fin\n",
            "  Downloading https://files.pythonhosted.org/packages/fe/bd/27f0066d596c87817b7d8f4a3533fdb666b1649007daee1965751adf07e8/yahoo_fin-0.8.4-py3-none-any.whl\n",
            "Installing collected packages: yahoo-fin\n",
            "Successfully installed yahoo-fin-0.8.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lhSKd0kz83_",
        "colab_type": "code",
        "outputId": "635e84d5-d6a8-496c-fb8f-835558f13464",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 817
        }
      },
      "source": [
        "pip install requests_html"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting requests_html\n",
            "  Downloading https://files.pythonhosted.org/packages/24/bc/a4380f09bab3a776182578ce6b2771e57259d0d4dbce178205779abdc347/requests_html-0.10.0-py3-none-any.whl\n",
            "Requirement already satisfied: bs4 in /usr/local/lib/python3.6/dist-packages (from requests_html) (0.0.1)\n",
            "Collecting w3lib\n",
            "  Downloading https://files.pythonhosted.org/packages/6a/45/1ba17c50a0bb16bd950c9c2b92ec60d40c8ebda9f3371ae4230c437120b6/w3lib-1.21.0-py2.py3-none-any.whl\n",
            "Collecting fake-useragent\n",
            "  Downloading https://files.pythonhosted.org/packages/d1/79/af647635d6968e2deb57a208d309f6069d31cb138066d7e821e575112a80/fake-useragent-0.1.11.tar.gz\n",
            "Collecting parse\n",
            "  Downloading https://files.pythonhosted.org/packages/4a/ea/9a16ff916752241aa80f1a5ec56dc6c6defc5d0e70af2d16904a9573367f/parse-1.14.0.tar.gz\n",
            "Collecting pyquery\n",
            "  Downloading https://files.pythonhosted.org/packages/78/43/95d42e386c61cb639d1a0b94f0c0b9f0b7d6b981ad3c043a836c8b5bc68b/pyquery-1.4.1-py2.py3-none-any.whl\n",
            "Collecting pyppeteer>=0.0.14\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b0/16/a5e8d617994cac605f972523bb25f12e3ff9c30baee29b4a9c50467229d9/pyppeteer-0.0.25.tar.gz (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 6.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from requests_html) (2.21.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from bs4->requests_html) (4.6.3)\n",
            "Requirement already satisfied: six>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from w3lib->requests_html) (1.12.0)\n",
            "Requirement already satisfied: lxml>=2.1 in /usr/local/lib/python3.6/dist-packages (from pyquery->requests_html) (4.2.6)\n",
            "Collecting cssselect>0.7.9\n",
            "  Downloading https://files.pythonhosted.org/packages/3b/d4/3b5c17f00cce85b9a1e6f91096e1cc8e8ede2e1be8e96b87ce1ed09e92c5/cssselect-1.1.0-py2.py3-none-any.whl\n",
            "Collecting pyee\n",
            "  Downloading https://files.pythonhosted.org/packages/d4/ce/a9a45667cdffbda2af63baa3ad1adca2feb80abf307d0dc91f9dfdfb3ec3/pyee-7.0.1-py2.py3-none-any.whl\n",
            "Collecting websockets\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/d9/856af84843912e2853b1b6e898ac8b802989fcf9ecf8e8445a1da263bf3b/websockets-8.1-cp36-cp36m-manylinux2010_x86_64.whl (78kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 8.9MB/s \n",
            "\u001b[?25hCollecting appdirs\n",
            "  Downloading https://files.pythonhosted.org/packages/56/eb/810e700ed1349edde4cbdc1b2a21e28cdf115f9faf263f6bbf8447c1abf3/appdirs-1.4.3-py2.py3-none-any.whl\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.6/dist-packages (from pyppeteer>=0.0.14->requests_html) (1.24.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pyppeteer>=0.0.14->requests_html) (4.28.1)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->requests_html) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->requests_html) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->requests_html) (3.0.4)\n",
            "Building wheels for collected packages: fake-useragent, parse, pyppeteer\n",
            "  Building wheel for fake-useragent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fake-useragent: filename=fake_useragent-0.1.11-cp36-none-any.whl size=13484 sha256=d1c36d0a63911d8e155132bf67ba1cf4ecb0b5f11548f0094e7627c348436159\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/63/09/d1dc15179f175357d3f5c00cbffbac37f9e8690d80545143ff\n",
            "  Building wheel for parse (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for parse: filename=parse-1.14.0-cp36-none-any.whl size=23463 sha256=edd5a0ed5846a470edbeb5f5db917ec471c3d6ca3414e240005e614aa9f6e86f\n",
            "  Stored in directory: /root/.cache/pip/wheels/d7/07/e0/b74bfdc1d434e73ef79e69e301e82a7825e0c070f7442beb61\n",
            "  Building wheel for pyppeteer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyppeteer: filename=pyppeteer-0.0.25-cp36-none-any.whl size=78360 sha256=58fee6febc1e77059b9b25183fe75a0b3efcb8096e0c6e8d9c9e81cce7e39d8b\n",
            "  Stored in directory: /root/.cache/pip/wheels/34/e0/5d/070e22eceecf7ecd5fa4b86bbc18c1c7d0b90e02e9b57f35eb\n",
            "Successfully built fake-useragent parse pyppeteer\n",
            "Installing collected packages: w3lib, fake-useragent, parse, cssselect, pyquery, pyee, websockets, appdirs, pyppeteer, requests-html\n",
            "Successfully installed appdirs-1.4.3 cssselect-1.1.0 fake-useragent-0.1.11 parse-1.14.0 pyee-7.0.1 pyppeteer-0.0.25 pyquery-1.4.1 requests-html-0.10.0 w3lib-1.21.0 websockets-8.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UW4YTlmz-_e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from yahoo_fin import stock_info as si\n",
        "from collections import deque\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import time\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZnZ10Tt0Bi-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Preparing the Dataset\n",
        "def load_data(ticker, n_steps=50, scale=True, shuffle=True, lookup_step=1, test_size=0.2,\n",
        "              feature_columns=['adjclose', 'volume', 'open', 'high', 'low']):\n",
        "  #see if ticker is already a loaded stock from yahoo finance\n",
        "  if isinstance(ticker, str):\n",
        "    #load it from yahoo_fin library\n",
        "    df = si.get_data(ticker)\n",
        "  elif isinstance(ticker, pd.DataFrame):\n",
        "    df = ticker\n",
        "\n",
        "  # this will contain all the elements we want to return from this function\n",
        "  result = {}\n",
        "  #we will also return original dataframe from this function\n",
        "  result['df'] = df.copy()\n",
        "  # check column passed is in the dataframe itself\n",
        "  for column in feature_columns:\n",
        "    assert column in df.columns\n",
        "\n",
        "  if scale:\n",
        "    column_scaler = {}\n",
        "    # scale the data(prices) from 0 to 1\n",
        "    for column in feature_columns:\n",
        "      scalar = preprocessing.MinMaxScaler()\n",
        "      df[column] = scalar.fit_transform(np.expand_dims(df[column].values, axis=1))\n",
        "      column_scaler[column] = scalar\n",
        "\n",
        "    # add the MinMaxScalar instances to the result returned\n",
        "    result['column_scaler'] = column_scaler\n",
        "\n",
        "  # add the target column(label) by shifting by lookup_step\n",
        "  df['future'] = df['adjclose'].shift(-lookup_step)\n",
        "\n",
        "  # last lookup_state columns contains NaN in future column\n",
        "  # get them before dropping NaN\n",
        "  last_sequence = np.array(df[feature_columns].tail(lookup_step))\n",
        "\n",
        "  #drop NaNs\n",
        "  df.dropna(inplace=True)\n",
        "\n",
        "  sequence_data = []\n",
        "  sequences = deque(maxlen=n_steps)\n",
        "  for entry, target in zip(df[feature_columns].values, df['future'].values):\n",
        "    sequences.append(entry)\n",
        "    if len(sequences) == n_steps:\n",
        "      sequence_data.append([np.array(sequences), target])\n",
        "\n",
        "  # get the last sequence by appending the last 'n_step' sequence with 'lookup_step' sequence\n",
        "  # for instance, if n_steps = 50 and lookup_step=10, last_sequence should beof 59(50+10-1) length\n",
        "  # this last_sequence will be used to predict in future dates that are not available in the dataset\n",
        "  last_sequence = list(sequences) + list(last_sequence)\n",
        "  # shift the last sequence by -1\n",
        "  last_sequence = np.array(pd.DataFrame(last_sequence).shift(-1).dropna())\n",
        "  # add to result\n",
        "  result['last_sequence'] = last_sequence\n",
        "  # construct the X's and Y's\n",
        "  X, y = [], []\n",
        "\n",
        "  for seq, target in sequence_data:\n",
        "    X.append(seq)\n",
        "    y.append(target)\n",
        "\n",
        "  # convert to numpy array\n",
        "  X = np.array(X)\n",
        "  y = np.array(y)\n",
        "\n",
        "  # reshape X to fit the neural network\n",
        "  X = X.reshape((X.shape[0], X.shape[2], X.shape[1]))\n",
        "\n",
        "  # split the dataset\n",
        "  result[\"X_train\"], result[\"X_test\"], result[\"y_train\"], result[\"y_test\"] = train_test_split(X,y,test_size= test_size, shuffle=shuffle)\n",
        "\n",
        "  # return the result\n",
        "  return result\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grgbpjLO0ER-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Model Creation\n",
        "\n",
        "def create_model(input_length, units=256, cell=LSTM, n_layers=2, dropout=0.3,\n",
        "                 loss=\"mean_absolute_error\", optimizer=\"rmsprop\"):\n",
        "  model = Sequential()\n",
        "  for i in range(n_layers):\n",
        "    if i==0:\n",
        "      #First layer\n",
        "      model.add(cell(units, return_sequences=True, input_shape=(None, input_length)))\n",
        "    elif i == n_layers - 1:\n",
        "      # Last layer\n",
        "      model.add(cell(units, return_sequences=False))\n",
        "    else:\n",
        "      # hidden layer\n",
        "      model.add(cell(units, return_sequences=True))\n",
        "\n",
        "    # add dropout after each layer\n",
        "    model.add(Dropout(dropout))\n",
        "\n",
        "  model.add(Dense(1, activation=\"linear\"))\n",
        "  model.compile(loss=loss, metrics=[\"mean_absolute_error\"], optimizer=optimizer)\n",
        "\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I66ZjJ1X1gWg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training the model\n",
        "# Window size or the sequence length\n",
        "N_STEPS = 50\n",
        "# Lookup step, 1 is the next day\n",
        "LOOKUP_STEP = 1\n",
        "# test ratio size, 0.2 is 20%\n",
        "TEST_SIZE = 0.2\n",
        "# features to use\n",
        "FEATURE_COLUMNS = [\"adjclose\", \"volume\", \"open\", \"high\", \"low\"]\n",
        "# date now\n",
        "date_now = time.strftime(\"%Y-%m-%d\")\n",
        "### model parameters\n",
        "N_LAYERS = 3\n",
        "# LSTM cell\n",
        "CELL = LSTM\n",
        "# 256 LSTM neurons\n",
        "UNITS = 256\n",
        "# 40% dropout\n",
        "DROPOUT = 0.4\n",
        "### training parameters\n",
        "# mean squared error loss\n",
        "LOSS = \"mse\"\n",
        "OPTIMIZER = \"rmsprop\"\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 300\n",
        "# Apple stock market\n",
        "ticker = \"AAPL\"\n",
        "ticker_data_filename = os.path.join(\"data\", f\"{ticker}_{date_now}.csv\")\n",
        "# model name to save\n",
        "model_name = f\"{date_now}_{ticker}-{LOSS}-{CELL.__name__}-seq-{N_STEPS}-step-{LOOKUP_STEP}-layers-{N_LAYERS}-units-{UNITS}\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mV87iqmC2_uI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create these folders if they does not exist\n",
        "if not os.path.isdir(\"results\"):\n",
        "    os.mkdir(\"results\")\n",
        "if not os.path.isdir(\"logs\"):\n",
        "    os.mkdir(\"logs\")\n",
        "if not os.path.isdir(\"data\"):\n",
        "    os.mkdir(\"data\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjUUdxY-3GKI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load the CSV file from disk (dataset) if it already exists (without downloading)\n",
        "if os.path.isfile(ticker_data_filename):\n",
        "    ticker = pd.read_csv(ticker_data_filename)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svAanBnE3W7u",
        "colab_type": "code",
        "outputId": "959dd7b1-ae2a-4f40-8ee0-f6b1639910af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# load the data\n",
        "\n",
        "data = load_data(ticker, N_STEPS, lookup_step=LOOKUP_STEP, test_size=TEST_SIZE, feature_columns=FEATURE_COLUMNS)\n",
        "if not os.path.isfile(ticker_data_filename):\n",
        "  # save the CSV file(dataset)\n",
        "  data[\"df\"] .to_csv(ticker_data_filename)\n",
        "\n",
        "# construct the model\n",
        "model = create_model(N_STEPS, loss=LOSS, units=UNITS, cell=CELL, n_layers=N_LAYERS, dropout=DROPOUT, optimizer=OPTIMIZER)\n",
        "\n",
        "#some tensorflow callbacks\n",
        "checkpointer = ModelCheckpoint(os.path.join(\"results\", model_name), save_best_only=True, verbose=1)\n",
        "tensorboard = TensorBoard(log_dir=os.path.join(\"logs\", model_name))\n",
        "\n",
        "history = model.fit(data[\"X_train\"], data[\"y_train\"],\n",
        "                    batch_size=BATCH_SIZE,\n",
        "                    epochs=EPOCHS,\n",
        "                    validation_data=(data[\"X_test\"], data[\"y_test\"]),\n",
        "                    callbacks=[checkpointer, tensorboard],\n",
        "                    verbose=1)\n",
        "\n",
        "model.save(os.path.join(\"results\", model_name) + \".h5\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Train on 7861 samples, validate on 1966 samples\n",
            "Epoch 1/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 0.0038 - mean_absolute_error: 0.0271\n",
            "Epoch 00001: val_loss improved from inf to 0.00051, saving model to results/2020-02-18_AAPL-mse-LSTM-seq-50-step-1-layers-3-units-256\n",
            "7861/7861 [==============================] - 28s 4ms/sample - loss: 0.0038 - mean_absolute_error: 0.0270 - val_loss: 5.0893e-04 - val_mean_absolute_error: 0.0134\n",
            "Epoch 2/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 0.0019 - mean_absolute_error: 0.0198\n",
            "Epoch 00002: val_loss improved from 0.00051 to 0.00034, saving model to results/2020-02-18_AAPL-mse-LSTM-seq-50-step-1-layers-3-units-256\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 0.0019 - mean_absolute_error: 0.0197 - val_loss: 3.4142e-04 - val_mean_absolute_error: 0.0079\n",
            "Epoch 3/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 0.0016 - mean_absolute_error: 0.0192\n",
            "Epoch 00003: val_loss did not improve from 0.00034\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 0.0016 - mean_absolute_error: 0.0193 - val_loss: 0.0031 - val_mean_absolute_error: 0.0298\n",
            "Epoch 4/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 0.0013 - mean_absolute_error: 0.0183\n",
            "Epoch 00004: val_loss did not improve from 0.00034\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 0.0013 - mean_absolute_error: 0.0185 - val_loss: 0.0057 - val_mean_absolute_error: 0.0385\n",
            "Epoch 5/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 0.0013 - mean_absolute_error: 0.0181\n",
            "Epoch 00005: val_loss did not improve from 0.00034\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 0.0013 - mean_absolute_error: 0.0182 - val_loss: 0.0010 - val_mean_absolute_error: 0.0209\n",
            "Epoch 6/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 0.0011 - mean_absolute_error: 0.0178\n",
            "Epoch 00006: val_loss did not improve from 0.00034\n",
            "7861/7861 [==============================] - 26s 3ms/sample - loss: 0.0011 - mean_absolute_error: 0.0178 - val_loss: 4.2542e-04 - val_mean_absolute_error: 0.0145\n",
            "Epoch 7/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 9.9442e-04 - mean_absolute_error: 0.0169\n",
            "Epoch 00007: val_loss did not improve from 0.00034\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 9.9093e-04 - mean_absolute_error: 0.0169 - val_loss: 5.2415e-04 - val_mean_absolute_error: 0.0122\n",
            "Epoch 8/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 8.3434e-04 - mean_absolute_error: 0.0162\n",
            "Epoch 00008: val_loss improved from 0.00034 to 0.00022, saving model to results/2020-02-18_AAPL-mse-LSTM-seq-50-step-1-layers-3-units-256\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 8.2967e-04 - mean_absolute_error: 0.0161 - val_loss: 2.1970e-04 - val_mean_absolute_error: 0.0062\n",
            "Epoch 9/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 8.8227e-04 - mean_absolute_error: 0.0162\n",
            "Epoch 00009: val_loss improved from 0.00022 to 0.00020, saving model to results/2020-02-18_AAPL-mse-LSTM-seq-50-step-1-layers-3-units-256\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 8.7853e-04 - mean_absolute_error: 0.0162 - val_loss: 2.0298e-04 - val_mean_absolute_error: 0.0070\n",
            "Epoch 10/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 8.0641e-04 - mean_absolute_error: 0.0153\n",
            "Epoch 00010: val_loss improved from 0.00020 to 0.00019, saving model to results/2020-02-18_AAPL-mse-LSTM-seq-50-step-1-layers-3-units-256\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 8.0218e-04 - mean_absolute_error: 0.0153 - val_loss: 1.8522e-04 - val_mean_absolute_error: 0.0055\n",
            "Epoch 11/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 7.5560e-04 - mean_absolute_error: 0.0151\n",
            "Epoch 00011: val_loss did not improve from 0.00019\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 7.5181e-04 - mean_absolute_error: 0.0150 - val_loss: 2.0858e-04 - val_mean_absolute_error: 0.0110\n",
            "Epoch 12/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 7.5511e-04 - mean_absolute_error: 0.0149\n",
            "Epoch 00012: val_loss improved from 0.00019 to 0.00012, saving model to results/2020-02-18_AAPL-mse-LSTM-seq-50-step-1-layers-3-units-256\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 7.5269e-04 - mean_absolute_error: 0.0149 - val_loss: 1.1782e-04 - val_mean_absolute_error: 0.0060\n",
            "Epoch 13/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 6.8592e-04 - mean_absolute_error: 0.0143\n",
            "Epoch 00013: val_loss did not improve from 0.00012\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 6.8405e-04 - mean_absolute_error: 0.0143 - val_loss: 2.0169e-04 - val_mean_absolute_error: 0.0109\n",
            "Epoch 14/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 6.7431e-04 - mean_absolute_error: 0.0144\n",
            "Epoch 00014: val_loss did not improve from 0.00012\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 6.7357e-04 - mean_absolute_error: 0.0144 - val_loss: 5.3484e-04 - val_mean_absolute_error: 0.0092\n",
            "Epoch 15/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 6.5584e-04 - mean_absolute_error: 0.0142\n",
            "Epoch 00015: val_loss did not improve from 0.00012\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 6.5263e-04 - mean_absolute_error: 0.0142 - val_loss: 1.5780e-04 - val_mean_absolute_error: 0.0047\n",
            "Epoch 16/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 6.1256e-04 - mean_absolute_error: 0.0140\n",
            "Epoch 00016: val_loss did not improve from 0.00012\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 6.0975e-04 - mean_absolute_error: 0.0140 - val_loss: 1.9377e-04 - val_mean_absolute_error: 0.0083\n",
            "Epoch 17/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 6.0562e-04 - mean_absolute_error: 0.0141\n",
            "Epoch 00017: val_loss did not improve from 0.00012\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 6.0985e-04 - mean_absolute_error: 0.0141 - val_loss: 5.6779e-04 - val_mean_absolute_error: 0.0127\n",
            "Epoch 18/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 5.7475e-04 - mean_absolute_error: 0.0137\n",
            "Epoch 00018: val_loss did not improve from 0.00012\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 5.8196e-04 - mean_absolute_error: 0.0137 - val_loss: 0.0017 - val_mean_absolute_error: 0.0198\n",
            "Epoch 19/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 5.0622e-04 - mean_absolute_error: 0.0131\n",
            "Epoch 00019: val_loss did not improve from 0.00012\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 5.0680e-04 - mean_absolute_error: 0.0131 - val_loss: 1.8434e-04 - val_mean_absolute_error: 0.0118\n",
            "Epoch 20/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 4.9308e-04 - mean_absolute_error: 0.0127\n",
            "Epoch 00020: val_loss did not improve from 0.00012\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 4.9142e-04 - mean_absolute_error: 0.0127 - val_loss: 1.3921e-04 - val_mean_absolute_error: 0.0086\n",
            "Epoch 21/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 4.7547e-04 - mean_absolute_error: 0.0127\n",
            "Epoch 00021: val_loss did not improve from 0.00012\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 4.7478e-04 - mean_absolute_error: 0.0127 - val_loss: 3.7020e-04 - val_mean_absolute_error: 0.0103\n",
            "Epoch 22/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 4.9491e-04 - mean_absolute_error: 0.0128\n",
            "Epoch 00022: val_loss improved from 0.00012 to 0.00012, saving model to results/2020-02-18_AAPL-mse-LSTM-seq-50-step-1-layers-3-units-256\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 4.9602e-04 - mean_absolute_error: 0.0129 - val_loss: 1.1577e-04 - val_mean_absolute_error: 0.0064\n",
            "Epoch 23/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 4.5211e-04 - mean_absolute_error: 0.0122\n",
            "Epoch 00023: val_loss did not improve from 0.00012\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 4.5136e-04 - mean_absolute_error: 0.0122 - val_loss: 1.5373e-04 - val_mean_absolute_error: 0.0108\n",
            "Epoch 24/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 4.9254e-04 - mean_absolute_error: 0.0127\n",
            "Epoch 00024: val_loss did not improve from 0.00012\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 4.9602e-04 - mean_absolute_error: 0.0127 - val_loss: 4.1233e-04 - val_mean_absolute_error: 0.0160\n",
            "Epoch 25/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 4.8282e-04 - mean_absolute_error: 0.0122\n",
            "Epoch 00025: val_loss did not improve from 0.00012\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 4.8384e-04 - mean_absolute_error: 0.0122 - val_loss: 2.9311e-04 - val_mean_absolute_error: 0.0086\n",
            "Epoch 26/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 4.2688e-04 - mean_absolute_error: 0.0120\n",
            "Epoch 00026: val_loss did not improve from 0.00012\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 4.2486e-04 - mean_absolute_error: 0.0120 - val_loss: 1.4432e-04 - val_mean_absolute_error: 0.0101\n",
            "Epoch 27/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 4.4174e-04 - mean_absolute_error: 0.0120\n",
            "Epoch 00027: val_loss did not improve from 0.00012\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 4.4152e-04 - mean_absolute_error: 0.0121 - val_loss: 3.4992e-04 - val_mean_absolute_error: 0.0132\n",
            "Epoch 28/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 4.2685e-04 - mean_absolute_error: 0.0122\n",
            "Epoch 00028: val_loss improved from 0.00012 to 0.00011, saving model to results/2020-02-18_AAPL-mse-LSTM-seq-50-step-1-layers-3-units-256\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 4.2616e-04 - mean_absolute_error: 0.0122 - val_loss: 1.1097e-04 - val_mean_absolute_error: 0.0069\n",
            "Epoch 29/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 4.2335e-04 - mean_absolute_error: 0.0118\n",
            "Epoch 00029: val_loss did not improve from 0.00011\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 4.2233e-04 - mean_absolute_error: 0.0118 - val_loss: 2.9979e-04 - val_mean_absolute_error: 0.0109\n",
            "Epoch 30/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 4.0426e-04 - mean_absolute_error: 0.0117\n",
            "Epoch 00030: val_loss improved from 0.00011 to 0.00009, saving model to results/2020-02-18_AAPL-mse-LSTM-seq-50-step-1-layers-3-units-256\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 4.0355e-04 - mean_absolute_error: 0.0117 - val_loss: 9.3140e-05 - val_mean_absolute_error: 0.0078\n",
            "Epoch 31/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 4.0841e-04 - mean_absolute_error: 0.0116\n",
            "Epoch 00031: val_loss improved from 0.00009 to 0.00008, saving model to results/2020-02-18_AAPL-mse-LSTM-seq-50-step-1-layers-3-units-256\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 4.0638e-04 - mean_absolute_error: 0.0116 - val_loss: 7.9476e-05 - val_mean_absolute_error: 0.0053\n",
            "Epoch 32/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 4.0672e-04 - mean_absolute_error: 0.0114\n",
            "Epoch 00032: val_loss did not improve from 0.00008\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 4.0826e-04 - mean_absolute_error: 0.0114 - val_loss: 9.7139e-05 - val_mean_absolute_error: 0.0082\n",
            "Epoch 33/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 3.8994e-04 - mean_absolute_error: 0.0114\n",
            "Epoch 00033: val_loss did not improve from 0.00008\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 3.9181e-04 - mean_absolute_error: 0.0114 - val_loss: 6.7314e-04 - val_mean_absolute_error: 0.0144\n",
            "Epoch 34/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 4.0237e-04 - mean_absolute_error: 0.0114\n",
            "Epoch 00034: val_loss did not improve from 0.00008\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 4.0309e-04 - mean_absolute_error: 0.0114 - val_loss: 2.5205e-04 - val_mean_absolute_error: 0.0109\n",
            "Epoch 35/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 3.7704e-04 - mean_absolute_error: 0.0111\n",
            "Epoch 00035: val_loss did not improve from 0.00008\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 3.8062e-04 - mean_absolute_error: 0.0111 - val_loss: 6.9778e-04 - val_mean_absolute_error: 0.0139\n",
            "Epoch 36/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 3.8737e-04 - mean_absolute_error: 0.0113\n",
            "Epoch 00036: val_loss improved from 0.00008 to 0.00006, saving model to results/2020-02-18_AAPL-mse-LSTM-seq-50-step-1-layers-3-units-256\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 3.8560e-04 - mean_absolute_error: 0.0113 - val_loss: 5.6448e-05 - val_mean_absolute_error: 0.0041\n",
            "Epoch 37/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 4.0404e-04 - mean_absolute_error: 0.0113\n",
            "Epoch 00037: val_loss did not improve from 0.00006\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 4.0543e-04 - mean_absolute_error: 0.0113 - val_loss: 1.4046e-04 - val_mean_absolute_error: 0.0096\n",
            "Epoch 38/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 3.5184e-04 - mean_absolute_error: 0.0110\n",
            "Epoch 00038: val_loss did not improve from 0.00006\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 3.5358e-04 - mean_absolute_error: 0.0110 - val_loss: 1.7368e-04 - val_mean_absolute_error: 0.0072\n",
            "Epoch 39/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 3.4882e-04 - mean_absolute_error: 0.0108\n",
            "Epoch 00039: val_loss did not improve from 0.00006\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 3.4939e-04 - mean_absolute_error: 0.0109 - val_loss: 3.1079e-04 - val_mean_absolute_error: 0.0102\n",
            "Epoch 40/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 3.7004e-04 - mean_absolute_error: 0.0110\n",
            "Epoch 00040: val_loss did not improve from 0.00006\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 3.6912e-04 - mean_absolute_error: 0.0110 - val_loss: 9.5197e-05 - val_mean_absolute_error: 0.0054\n",
            "Epoch 41/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 3.3319e-04 - mean_absolute_error: 0.0108\n",
            "Epoch 00041: val_loss did not improve from 0.00006\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 3.3807e-04 - mean_absolute_error: 0.0108 - val_loss: 4.8058e-04 - val_mean_absolute_error: 0.0114\n",
            "Epoch 42/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 3.6675e-04 - mean_absolute_error: 0.0109\n",
            "Epoch 00042: val_loss did not improve from 0.00006\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 3.6512e-04 - mean_absolute_error: 0.0109 - val_loss: 7.4283e-05 - val_mean_absolute_error: 0.0070\n",
            "Epoch 43/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 3.5580e-04 - mean_absolute_error: 0.0109\n",
            "Epoch 00043: val_loss did not improve from 0.00006\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 3.5527e-04 - mean_absolute_error: 0.0109 - val_loss: 6.9255e-05 - val_mean_absolute_error: 0.0046\n",
            "Epoch 44/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 3.3693e-04 - mean_absolute_error: 0.0106\n",
            "Epoch 00044: val_loss did not improve from 0.00006\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 3.3826e-04 - mean_absolute_error: 0.0106 - val_loss: 2.6767e-04 - val_mean_absolute_error: 0.0122\n",
            "Epoch 45/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 3.2686e-04 - mean_absolute_error: 0.0106\n",
            "Epoch 00045: val_loss did not improve from 0.00006\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 3.2642e-04 - mean_absolute_error: 0.0106 - val_loss: 1.4070e-04 - val_mean_absolute_error: 0.0078\n",
            "Epoch 46/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 3.5278e-04 - mean_absolute_error: 0.0108\n",
            "Epoch 00046: val_loss did not improve from 0.00006\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 3.5155e-04 - mean_absolute_error: 0.0108 - val_loss: 1.7010e-04 - val_mean_absolute_error: 0.0093\n",
            "Epoch 47/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 3.3961e-04 - mean_absolute_error: 0.0105\n",
            "Epoch 00047: val_loss improved from 0.00006 to 0.00005, saving model to results/2020-02-18_AAPL-mse-LSTM-seq-50-step-1-layers-3-units-256\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 3.3949e-04 - mean_absolute_error: 0.0105 - val_loss: 5.2632e-05 - val_mean_absolute_error: 0.0044\n",
            "Epoch 48/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 3.2706e-04 - mean_absolute_error: 0.0105\n",
            "Epoch 00048: val_loss did not improve from 0.00005\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 3.2782e-04 - mean_absolute_error: 0.0105 - val_loss: 1.5686e-04 - val_mean_absolute_error: 0.0053\n",
            "Epoch 49/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 3.4461e-04 - mean_absolute_error: 0.0105\n",
            "Epoch 00049: val_loss did not improve from 0.00005\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 3.4384e-04 - mean_absolute_error: 0.0105 - val_loss: 3.1793e-04 - val_mean_absolute_error: 0.0118\n",
            "Epoch 50/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 3.2105e-04 - mean_absolute_error: 0.0104\n",
            "Epoch 00050: val_loss did not improve from 0.00005\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 3.2463e-04 - mean_absolute_error: 0.0104 - val_loss: 3.8154e-04 - val_mean_absolute_error: 0.0109\n",
            "Epoch 51/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 3.2980e-04 - mean_absolute_error: 0.0106\n",
            "Epoch 00051: val_loss did not improve from 0.00005\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 3.2912e-04 - mean_absolute_error: 0.0105 - val_loss: 5.8948e-05 - val_mean_absolute_error: 0.0040\n",
            "Epoch 52/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 3.1956e-04 - mean_absolute_error: 0.0104\n",
            "Epoch 00052: val_loss did not improve from 0.00005\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 3.1904e-04 - mean_absolute_error: 0.0104 - val_loss: 6.8605e-05 - val_mean_absolute_error: 0.0037\n",
            "Epoch 53/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 3.2151e-04 - mean_absolute_error: 0.0104\n",
            "Epoch 00053: val_loss did not improve from 0.00005\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 3.2088e-04 - mean_absolute_error: 0.0104 - val_loss: 2.4716e-04 - val_mean_absolute_error: 0.0093\n",
            "Epoch 54/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 2.9622e-04 - mean_absolute_error: 0.0101\n",
            "Epoch 00054: val_loss did not improve from 0.00005\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 2.9563e-04 - mean_absolute_error: 0.0101 - val_loss: 1.5458e-04 - val_mean_absolute_error: 0.0096\n",
            "Epoch 55/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 2.9854e-04 - mean_absolute_error: 0.0102\n",
            "Epoch 00055: val_loss did not improve from 0.00005\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 2.9821e-04 - mean_absolute_error: 0.0102 - val_loss: 2.6636e-04 - val_mean_absolute_error: 0.0085\n",
            "Epoch 56/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 3.3420e-04 - mean_absolute_error: 0.0104\n",
            "Epoch 00056: val_loss did not improve from 0.00005\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 3.3310e-04 - mean_absolute_error: 0.0104 - val_loss: 6.6785e-05 - val_mean_absolute_error: 0.0050\n",
            "Epoch 57/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 3.1054e-04 - mean_absolute_error: 0.0102\n",
            "Epoch 00057: val_loss did not improve from 0.00005\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 3.0995e-04 - mean_absolute_error: 0.0102 - val_loss: 3.2555e-04 - val_mean_absolute_error: 0.0091\n",
            "Epoch 58/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 3.0112e-04 - mean_absolute_error: 0.0102\n",
            "Epoch 00058: val_loss did not improve from 0.00005\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 3.0077e-04 - mean_absolute_error: 0.0102 - val_loss: 1.0505e-04 - val_mean_absolute_error: 0.0084\n",
            "Epoch 59/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 3.2922e-04 - mean_absolute_error: 0.0103\n",
            "Epoch 00059: val_loss did not improve from 0.00005\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 3.3026e-04 - mean_absolute_error: 0.0103 - val_loss: 2.4672e-04 - val_mean_absolute_error: 0.0096\n",
            "Epoch 60/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 3.1974e-04 - mean_absolute_error: 0.0100\n",
            "Epoch 00060: val_loss did not improve from 0.00005\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 3.1986e-04 - mean_absolute_error: 0.0100 - val_loss: 2.1771e-04 - val_mean_absolute_error: 0.0101\n",
            "Epoch 61/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 3.0808e-04 - mean_absolute_error: 0.0103\n",
            "Epoch 00061: val_loss did not improve from 0.00005\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 3.0710e-04 - mean_absolute_error: 0.0103 - val_loss: 2.3853e-04 - val_mean_absolute_error: 0.0114\n",
            "Epoch 62/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 3.0759e-04 - mean_absolute_error: 0.0103\n",
            "Epoch 00062: val_loss did not improve from 0.00005\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 3.0789e-04 - mean_absolute_error: 0.0103 - val_loss: 9.9402e-05 - val_mean_absolute_error: 0.0053\n",
            "Epoch 63/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 2.8166e-04 - mean_absolute_error: 0.0099\n",
            "Epoch 00063: val_loss did not improve from 0.00005\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 2.8106e-04 - mean_absolute_error: 0.0098 - val_loss: 5.3515e-05 - val_mean_absolute_error: 0.0043\n",
            "Epoch 64/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 2.7965e-04 - mean_absolute_error: 0.0099\n",
            "Epoch 00064: val_loss did not improve from 0.00005\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 2.7976e-04 - mean_absolute_error: 0.0099 - val_loss: 8.8597e-05 - val_mean_absolute_error: 0.0066\n",
            "Epoch 65/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 3.0893e-04 - mean_absolute_error: 0.0102\n",
            "Epoch 00065: val_loss did not improve from 0.00005\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 3.1175e-04 - mean_absolute_error: 0.0102 - val_loss: 1.2222e-04 - val_mean_absolute_error: 0.0070\n",
            "Epoch 66/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 2.9375e-04 - mean_absolute_error: 0.0102\n",
            "Epoch 00066: val_loss did not improve from 0.00005\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 2.9236e-04 - mean_absolute_error: 0.0102 - val_loss: 6.6496e-05 - val_mean_absolute_error: 0.0043\n",
            "Epoch 67/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 2.8443e-04 - mean_absolute_error: 0.0099\n",
            "Epoch 00067: val_loss did not improve from 0.00005\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 2.8386e-04 - mean_absolute_error: 0.0099 - val_loss: 1.1441e-04 - val_mean_absolute_error: 0.0068\n",
            "Epoch 68/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 2.7821e-04 - mean_absolute_error: 0.0097\n",
            "Epoch 00068: val_loss did not improve from 0.00005\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 2.7836e-04 - mean_absolute_error: 0.0097 - val_loss: 8.0304e-05 - val_mean_absolute_error: 0.0052\n",
            "Epoch 69/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 3.0266e-04 - mean_absolute_error: 0.0100\n",
            "Epoch 00069: val_loss did not improve from 0.00005\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 3.0224e-04 - mean_absolute_error: 0.0100 - val_loss: 1.0184e-04 - val_mean_absolute_error: 0.0058\n",
            "Epoch 70/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 2.9478e-04 - mean_absolute_error: 0.0100\n",
            "Epoch 00070: val_loss did not improve from 0.00005\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 2.9964e-04 - mean_absolute_error: 0.0100 - val_loss: 1.3133e-04 - val_mean_absolute_error: 0.0085\n",
            "Epoch 71/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 3.0628e-04 - mean_absolute_error: 0.0100\n",
            "Epoch 00071: val_loss did not improve from 0.00005\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 3.0566e-04 - mean_absolute_error: 0.0100 - val_loss: 8.5022e-05 - val_mean_absolute_error: 0.0055\n",
            "Epoch 72/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 3.0740e-04 - mean_absolute_error: 0.0099\n",
            "Epoch 00072: val_loss did not improve from 0.00005\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 3.0589e-04 - mean_absolute_error: 0.0099 - val_loss: 7.6342e-05 - val_mean_absolute_error: 0.0042\n",
            "Epoch 73/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 2.5459e-04 - mean_absolute_error: 0.0096\n",
            "Epoch 00073: val_loss did not improve from 0.00005\n",
            "7861/7861 [==============================] - 24s 3ms/sample - loss: 2.5540e-04 - mean_absolute_error: 0.0096 - val_loss: 9.8352e-05 - val_mean_absolute_error: 0.0059\n",
            "Epoch 74/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 2.9348e-04 - mean_absolute_error: 0.0098\n",
            "Epoch 00074: val_loss did not improve from 0.00005\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 2.9237e-04 - mean_absolute_error: 0.0098 - val_loss: 1.4282e-04 - val_mean_absolute_error: 0.0055\n",
            "Epoch 75/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 3.0597e-04 - mean_absolute_error: 0.0098\n",
            "Epoch 00075: val_loss did not improve from 0.00005\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 3.0771e-04 - mean_absolute_error: 0.0099 - val_loss: 9.5337e-05 - val_mean_absolute_error: 0.0060\n",
            "Epoch 76/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 2.6966e-04 - mean_absolute_error: 0.0096\n",
            "Epoch 00076: val_loss did not improve from 0.00005\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 2.7066e-04 - mean_absolute_error: 0.0096 - val_loss: 3.3396e-04 - val_mean_absolute_error: 0.0107\n",
            "Epoch 77/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 2.9126e-04 - mean_absolute_error: 0.0098\n",
            "Epoch 00077: val_loss did not improve from 0.00005\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 2.9051e-04 - mean_absolute_error: 0.0098 - val_loss: 7.2662e-05 - val_mean_absolute_error: 0.0049\n",
            "Epoch 78/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 2.6754e-04 - mean_absolute_error: 0.0096\n",
            "Epoch 00078: val_loss did not improve from 0.00005\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 2.6673e-04 - mean_absolute_error: 0.0096 - val_loss: 1.3523e-04 - val_mean_absolute_error: 0.0087\n",
            "Epoch 79/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 2.7761e-04 - mean_absolute_error: 0.0096\n",
            "Epoch 00079: val_loss improved from 0.00005 to 0.00005, saving model to results/2020-02-18_AAPL-mse-LSTM-seq-50-step-1-layers-3-units-256\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 2.7840e-04 - mean_absolute_error: 0.0096 - val_loss: 5.0101e-05 - val_mean_absolute_error: 0.0044\n",
            "Epoch 80/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 2.6733e-04 - mean_absolute_error: 0.0096\n",
            "Epoch 00080: val_loss improved from 0.00005 to 0.00005, saving model to results/2020-02-18_AAPL-mse-LSTM-seq-50-step-1-layers-3-units-256\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 2.6817e-04 - mean_absolute_error: 0.0096 - val_loss: 4.6766e-05 - val_mean_absolute_error: 0.0044\n",
            "Epoch 81/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 2.5114e-04 - mean_absolute_error: 0.0095\n",
            "Epoch 00081: val_loss did not improve from 0.00005\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 2.5231e-04 - mean_absolute_error: 0.0095 - val_loss: 2.0223e-04 - val_mean_absolute_error: 0.0099\n",
            "Epoch 82/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 2.8321e-04 - mean_absolute_error: 0.0095\n",
            "Epoch 00082: val_loss did not improve from 0.00005\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 2.8190e-04 - mean_absolute_error: 0.0095 - val_loss: 6.6618e-05 - val_mean_absolute_error: 0.0067\n",
            "Epoch 83/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 2.3979e-04 - mean_absolute_error: 0.0093\n",
            "Epoch 00083: val_loss did not improve from 0.00005\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 2.4010e-04 - mean_absolute_error: 0.0093 - val_loss: 4.7663e-05 - val_mean_absolute_error: 0.0040\n",
            "Epoch 84/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 2.7886e-04 - mean_absolute_error: 0.0096\n",
            "Epoch 00084: val_loss did not improve from 0.00005\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 2.7804e-04 - mean_absolute_error: 0.0096 - val_loss: 1.1460e-04 - val_mean_absolute_error: 0.0088\n",
            "Epoch 85/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 2.4565e-04 - mean_absolute_error: 0.0092\n",
            "Epoch 00085: val_loss did not improve from 0.00005\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 2.4675e-04 - mean_absolute_error: 0.0092 - val_loss: 1.5416e-04 - val_mean_absolute_error: 0.0068\n",
            "Epoch 86/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 2.7812e-04 - mean_absolute_error: 0.0098\n",
            "Epoch 00086: val_loss did not improve from 0.00005\n",
            "7861/7861 [==============================] - 26s 3ms/sample - loss: 2.7698e-04 - mean_absolute_error: 0.0097 - val_loss: 9.1918e-05 - val_mean_absolute_error: 0.0052\n",
            "Epoch 87/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 2.5562e-04 - mean_absolute_error: 0.0093\n",
            "Epoch 00087: val_loss did not improve from 0.00005\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 2.5509e-04 - mean_absolute_error: 0.0093 - val_loss: 4.7986e-05 - val_mean_absolute_error: 0.0046\n",
            "Epoch 88/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 2.6588e-04 - mean_absolute_error: 0.0096\n",
            "Epoch 00088: val_loss did not improve from 0.00005\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 2.6483e-04 - mean_absolute_error: 0.0096 - val_loss: 5.2838e-05 - val_mean_absolute_error: 0.0052\n",
            "Epoch 89/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 2.4548e-04 - mean_absolute_error: 0.0093\n",
            "Epoch 00089: val_loss did not improve from 0.00005\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 2.4525e-04 - mean_absolute_error: 0.0093 - val_loss: 5.0560e-05 - val_mean_absolute_error: 0.0043\n",
            "Epoch 90/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 2.6583e-04 - mean_absolute_error: 0.0095\n",
            "Epoch 00090: val_loss did not improve from 0.00005\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 2.6857e-04 - mean_absolute_error: 0.0095 - val_loss: 5.5297e-05 - val_mean_absolute_error: 0.0048\n",
            "Epoch 91/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 2.5460e-04 - mean_absolute_error: 0.0094\n",
            "Epoch 00091: val_loss did not improve from 0.00005\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 2.5503e-04 - mean_absolute_error: 0.0094 - val_loss: 5.1423e-05 - val_mean_absolute_error: 0.0034\n",
            "Epoch 92/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 2.5533e-04 - mean_absolute_error: 0.0094\n",
            "Epoch 00092: val_loss did not improve from 0.00005\n",
            "7861/7861 [==============================] - 24s 3ms/sample - loss: 2.5433e-04 - mean_absolute_error: 0.0094 - val_loss: 4.9649e-05 - val_mean_absolute_error: 0.0037\n",
            "Epoch 93/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 2.3877e-04 - mean_absolute_error: 0.0092\n",
            "Epoch 00093: val_loss did not improve from 0.00005\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 2.4003e-04 - mean_absolute_error: 0.0092 - val_loss: 1.4562e-04 - val_mean_absolute_error: 0.0080\n",
            "Epoch 94/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 2.5582e-04 - mean_absolute_error: 0.0094\n",
            "Epoch 00094: val_loss did not improve from 0.00005\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 2.5546e-04 - mean_absolute_error: 0.0094 - val_loss: 1.4496e-04 - val_mean_absolute_error: 0.0090\n",
            "Epoch 95/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 2.3738e-04 - mean_absolute_error: 0.0092\n",
            "Epoch 00095: val_loss did not improve from 0.00005\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 2.3919e-04 - mean_absolute_error: 0.0092 - val_loss: 1.1410e-04 - val_mean_absolute_error: 0.0054\n",
            "Epoch 96/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 2.8030e-04 - mean_absolute_error: 0.0095\n",
            "Epoch 00096: val_loss did not improve from 0.00005\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 2.7911e-04 - mean_absolute_error: 0.0095 - val_loss: 6.0795e-05 - val_mean_absolute_error: 0.0048\n",
            "Epoch 97/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 2.6723e-04 - mean_absolute_error: 0.0094\n",
            "Epoch 00097: val_loss did not improve from 0.00005\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 2.6857e-04 - mean_absolute_error: 0.0094 - val_loss: 2.4738e-04 - val_mean_absolute_error: 0.0106\n",
            "Epoch 98/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 2.5401e-04 - mean_absolute_error: 0.0094\n",
            "Epoch 00098: val_loss did not improve from 0.00005\n",
            "7861/7861 [==============================] - 24s 3ms/sample - loss: 2.5303e-04 - mean_absolute_error: 0.0094 - val_loss: 7.6337e-05 - val_mean_absolute_error: 0.0059\n",
            "Epoch 99/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 2.4239e-04 - mean_absolute_error: 0.0092\n",
            "Epoch 00099: val_loss did not improve from 0.00005\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 2.4149e-04 - mean_absolute_error: 0.0092 - val_loss: 7.6546e-05 - val_mean_absolute_error: 0.0044\n",
            "Epoch 100/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 2.4208e-04 - mean_absolute_error: 0.0093\n",
            "Epoch 00100: val_loss did not improve from 0.00005\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 2.4156e-04 - mean_absolute_error: 0.0092 - val_loss: 5.5627e-05 - val_mean_absolute_error: 0.0043\n",
            "Epoch 101/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 2.4662e-04 - mean_absolute_error: 0.0092\n",
            "Epoch 00101: val_loss did not improve from 0.00005\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 2.4548e-04 - mean_absolute_error: 0.0092 - val_loss: 9.7388e-05 - val_mean_absolute_error: 0.0040\n",
            "Epoch 102/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 2.5884e-04 - mean_absolute_error: 0.0092\n",
            "Epoch 00102: val_loss did not improve from 0.00005\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 2.5864e-04 - mean_absolute_error: 0.0092 - val_loss: 1.3532e-04 - val_mean_absolute_error: 0.0062\n",
            "Epoch 103/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 2.6078e-04 - mean_absolute_error: 0.0093\n",
            "Epoch 00103: val_loss did not improve from 0.00005\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 2.6159e-04 - mean_absolute_error: 0.0093 - val_loss: 2.4291e-04 - val_mean_absolute_error: 0.0110\n",
            "Epoch 104/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 2.4673e-04 - mean_absolute_error: 0.0092\n",
            "Epoch 00104: val_loss did not improve from 0.00005\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 2.4765e-04 - mean_absolute_error: 0.0092 - val_loss: 1.6783e-04 - val_mean_absolute_error: 0.0097\n",
            "Epoch 105/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 2.6757e-04 - mean_absolute_error: 0.0094\n",
            "Epoch 00105: val_loss did not improve from 0.00005\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 2.6830e-04 - mean_absolute_error: 0.0094 - val_loss: 4.7727e-05 - val_mean_absolute_error: 0.0053\n",
            "Epoch 106/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 2.5287e-04 - mean_absolute_error: 0.0092\n",
            "Epoch 00106: val_loss improved from 0.00005 to 0.00004, saving model to results/2020-02-18_AAPL-mse-LSTM-seq-50-step-1-layers-3-units-256\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 2.5164e-04 - mean_absolute_error: 0.0092 - val_loss: 4.1627e-05 - val_mean_absolute_error: 0.0033\n",
            "Epoch 107/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 2.5614e-04 - mean_absolute_error: 0.0092\n",
            "Epoch 00107: val_loss did not improve from 0.00004\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 2.5626e-04 - mean_absolute_error: 0.0092 - val_loss: 8.9929e-05 - val_mean_absolute_error: 0.0044\n",
            "Epoch 108/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 2.5035e-04 - mean_absolute_error: 0.0091\n",
            "Epoch 00108: val_loss did not improve from 0.00004\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 2.5033e-04 - mean_absolute_error: 0.0091 - val_loss: 4.9400e-05 - val_mean_absolute_error: 0.0037\n",
            "Epoch 109/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 2.4626e-04 - mean_absolute_error: 0.0092\n",
            "Epoch 00109: val_loss did not improve from 0.00004\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 2.4645e-04 - mean_absolute_error: 0.0092 - val_loss: 4.4049e-05 - val_mean_absolute_error: 0.0042\n",
            "Epoch 110/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 2.4895e-04 - mean_absolute_error: 0.0092\n",
            "Epoch 00110: val_loss did not improve from 0.00004\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 2.4823e-04 - mean_absolute_error: 0.0092 - val_loss: 5.0766e-05 - val_mean_absolute_error: 0.0046\n",
            "Epoch 111/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 2.3658e-04 - mean_absolute_error: 0.0091\n",
            "Epoch 00111: val_loss did not improve from 0.00004\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 2.3608e-04 - mean_absolute_error: 0.0091 - val_loss: 5.5469e-05 - val_mean_absolute_error: 0.0037\n",
            "Epoch 112/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 2.2723e-04 - mean_absolute_error: 0.0091\n",
            "Epoch 00112: val_loss did not improve from 0.00004\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 2.2702e-04 - mean_absolute_error: 0.0090 - val_loss: 1.7626e-04 - val_mean_absolute_error: 0.0069\n",
            "Epoch 113/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 2.5223e-04 - mean_absolute_error: 0.0093\n",
            "Epoch 00113: val_loss did not improve from 0.00004\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 2.5131e-04 - mean_absolute_error: 0.0092 - val_loss: 8.1647e-05 - val_mean_absolute_error: 0.0039\n",
            "Epoch 114/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 2.4407e-04 - mean_absolute_error: 0.0090\n",
            "Epoch 00114: val_loss did not improve from 0.00004\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 2.4452e-04 - mean_absolute_error: 0.0090 - val_loss: 1.3967e-04 - val_mean_absolute_error: 0.0083\n",
            "Epoch 115/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 2.2663e-04 - mean_absolute_error: 0.0090\n",
            "Epoch 00115: val_loss did not improve from 0.00004\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 2.2673e-04 - mean_absolute_error: 0.0090 - val_loss: 1.1556e-04 - val_mean_absolute_error: 0.0040\n",
            "Epoch 116/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 2.2800e-04 - mean_absolute_error: 0.0090\n",
            "Epoch 00116: val_loss did not improve from 0.00004\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 2.2823e-04 - mean_absolute_error: 0.0090 - val_loss: 4.9208e-05 - val_mean_absolute_error: 0.0050\n",
            "Epoch 117/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 2.5545e-04 - mean_absolute_error: 0.0091\n",
            "Epoch 00117: val_loss did not improve from 0.00004\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 2.5602e-04 - mean_absolute_error: 0.0091 - val_loss: 3.6015e-04 - val_mean_absolute_error: 0.0102\n",
            "Epoch 118/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 2.5123e-04 - mean_absolute_error: 0.0092\n",
            "Epoch 00118: val_loss did not improve from 0.00004\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 2.5191e-04 - mean_absolute_error: 0.0092 - val_loss: 7.5336e-05 - val_mean_absolute_error: 0.0059\n",
            "Epoch 119/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 2.2054e-04 - mean_absolute_error: 0.0088\n",
            "Epoch 00119: val_loss did not improve from 0.00004\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 2.2069e-04 - mean_absolute_error: 0.0088 - val_loss: 9.9773e-05 - val_mean_absolute_error: 0.0091\n",
            "Epoch 120/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 2.3193e-04 - mean_absolute_error: 0.0091\n",
            "Epoch 00120: val_loss did not improve from 0.00004\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 2.3101e-04 - mean_absolute_error: 0.0091 - val_loss: 5.6347e-05 - val_mean_absolute_error: 0.0036\n",
            "Epoch 121/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 2.2384e-04 - mean_absolute_error: 0.0089\n",
            "Epoch 00121: val_loss did not improve from 0.00004\n",
            "7861/7861 [==============================] - 24s 3ms/sample - loss: 2.2353e-04 - mean_absolute_error: 0.0089 - val_loss: 8.6157e-05 - val_mean_absolute_error: 0.0040\n",
            "Epoch 122/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 2.3891e-04 - mean_absolute_error: 0.0090\n",
            "Epoch 00122: val_loss did not improve from 0.00004\n",
            "7861/7861 [==============================] - 24s 3ms/sample - loss: 2.3972e-04 - mean_absolute_error: 0.0091 - val_loss: 1.6996e-04 - val_mean_absolute_error: 0.0108\n",
            "Epoch 123/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 2.5227e-04 - mean_absolute_error: 0.0092\n",
            "Epoch 00123: val_loss did not improve from 0.00004\n",
            "7861/7861 [==============================] - 24s 3ms/sample - loss: 2.5171e-04 - mean_absolute_error: 0.0092 - val_loss: 4.7872e-05 - val_mean_absolute_error: 0.0039\n",
            "Epoch 124/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 2.3827e-04 - mean_absolute_error: 0.0091\n",
            "Epoch 00124: val_loss did not improve from 0.00004\n",
            "7861/7861 [==============================] - 24s 3ms/sample - loss: 2.3890e-04 - mean_absolute_error: 0.0091 - val_loss: 3.4646e-04 - val_mean_absolute_error: 0.0093\n",
            "Epoch 125/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 2.4789e-04 - mean_absolute_error: 0.0092\n",
            "Epoch 00125: val_loss did not improve from 0.00004\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 2.4841e-04 - mean_absolute_error: 0.0092 - val_loss: 2.2101e-04 - val_mean_absolute_error: 0.0085\n",
            "Epoch 126/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 2.2888e-04 - mean_absolute_error: 0.0090\n",
            "Epoch 00126: val_loss did not improve from 0.00004\n",
            "7861/7861 [==============================] - 24s 3ms/sample - loss: 2.2771e-04 - mean_absolute_error: 0.0089 - val_loss: 4.9759e-05 - val_mean_absolute_error: 0.0038\n",
            "Epoch 127/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 2.2119e-04 - mean_absolute_error: 0.0089\n",
            "Epoch 00127: val_loss did not improve from 0.00004\n",
            "7861/7861 [==============================] - 24s 3ms/sample - loss: 2.2070e-04 - mean_absolute_error: 0.0089 - val_loss: 7.0202e-05 - val_mean_absolute_error: 0.0068\n",
            "Epoch 128/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 2.3759e-04 - mean_absolute_error: 0.0090\n",
            "Epoch 00128: val_loss did not improve from 0.00004\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 2.3843e-04 - mean_absolute_error: 0.0090 - val_loss: 8.5477e-05 - val_mean_absolute_error: 0.0078\n",
            "Epoch 129/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 2.2711e-04 - mean_absolute_error: 0.0091\n",
            "Epoch 00129: val_loss did not improve from 0.00004\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 2.2888e-04 - mean_absolute_error: 0.0092 - val_loss: 5.8705e-05 - val_mean_absolute_error: 0.0041\n",
            "Epoch 130/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 2.2991e-04 - mean_absolute_error: 0.0089\n",
            "Epoch 00130: val_loss did not improve from 0.00004\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 2.2987e-04 - mean_absolute_error: 0.0089 - val_loss: 6.7026e-05 - val_mean_absolute_error: 0.0060\n",
            "Epoch 131/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 2.3190e-04 - mean_absolute_error: 0.0089\n",
            "Epoch 00131: val_loss did not improve from 0.00004\n",
            "7861/7861 [==============================] - 24s 3ms/sample - loss: 2.3103e-04 - mean_absolute_error: 0.0089 - val_loss: 9.2525e-05 - val_mean_absolute_error: 0.0054\n",
            "Epoch 132/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 2.1262e-04 - mean_absolute_error: 0.0088\n",
            "Epoch 00132: val_loss did not improve from 0.00004\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 2.1225e-04 - mean_absolute_error: 0.0088 - val_loss: 1.7018e-04 - val_mean_absolute_error: 0.0080\n",
            "Epoch 133/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 2.1409e-04 - mean_absolute_error: 0.0088\n",
            "Epoch 00133: val_loss improved from 0.00004 to 0.00004, saving model to results/2020-02-18_AAPL-mse-LSTM-seq-50-step-1-layers-3-units-256\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 2.1473e-04 - mean_absolute_error: 0.0088 - val_loss: 3.9310e-05 - val_mean_absolute_error: 0.0040\n",
            "Epoch 134/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 2.1939e-04 - mean_absolute_error: 0.0088\n",
            "Epoch 00134: val_loss did not improve from 0.00004\n",
            "7861/7861 [==============================] - 24s 3ms/sample - loss: 2.2020e-04 - mean_absolute_error: 0.0088 - val_loss: 2.4029e-04 - val_mean_absolute_error: 0.0077\n",
            "Epoch 135/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 2.3713e-04 - mean_absolute_error: 0.0090\n",
            "Epoch 00135: val_loss did not improve from 0.00004\n",
            "7861/7861 [==============================] - 28s 4ms/sample - loss: 2.3681e-04 - mean_absolute_error: 0.0090 - val_loss: 5.0833e-05 - val_mean_absolute_error: 0.0045\n",
            "Epoch 136/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 2.2150e-04 - mean_absolute_error: 0.0088\n",
            "Epoch 00136: val_loss did not improve from 0.00004\n",
            "7861/7861 [==============================] - 24s 3ms/sample - loss: 2.2078e-04 - mean_absolute_error: 0.0088 - val_loss: 4.2881e-05 - val_mean_absolute_error: 0.0041\n",
            "Epoch 137/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 2.3379e-04 - mean_absolute_error: 0.0089\n",
            "Epoch 00137: val_loss did not improve from 0.00004\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 2.3278e-04 - mean_absolute_error: 0.0089 - val_loss: 8.5021e-05 - val_mean_absolute_error: 0.0052\n",
            "Epoch 138/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 2.2997e-04 - mean_absolute_error: 0.0088\n",
            "Epoch 00138: val_loss did not improve from 0.00004\n",
            "7861/7861 [==============================] - 24s 3ms/sample - loss: 2.3011e-04 - mean_absolute_error: 0.0088 - val_loss: 1.0388e-04 - val_mean_absolute_error: 0.0054\n",
            "Epoch 139/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 2.2447e-04 - mean_absolute_error: 0.0088\n",
            "Epoch 00139: val_loss did not improve from 0.00004\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 2.2384e-04 - mean_absolute_error: 0.0088 - val_loss: 5.2283e-05 - val_mean_absolute_error: 0.0032\n",
            "Epoch 140/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 2.4506e-04 - mean_absolute_error: 0.0091\n",
            "Epoch 00140: val_loss did not improve from 0.00004\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 2.4415e-04 - mean_absolute_error: 0.0091 - val_loss: 1.1057e-04 - val_mean_absolute_error: 0.0046\n",
            "Epoch 141/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 2.0173e-04 - mean_absolute_error: 0.0086\n",
            "Epoch 00141: val_loss did not improve from 0.00004\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 2.0235e-04 - mean_absolute_error: 0.0086 - val_loss: 1.8002e-04 - val_mean_absolute_error: 0.0075\n",
            "Epoch 142/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 2.3237e-04 - mean_absolute_error: 0.0087\n",
            "Epoch 00142: val_loss did not improve from 0.00004\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 2.3380e-04 - mean_absolute_error: 0.0087 - val_loss: 1.0914e-04 - val_mean_absolute_error: 0.0069\n",
            "Epoch 143/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 2.1303e-04 - mean_absolute_error: 0.0087\n",
            "Epoch 00143: val_loss did not improve from 0.00004\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 2.1283e-04 - mean_absolute_error: 0.0087 - val_loss: 1.5372e-04 - val_mean_absolute_error: 0.0073\n",
            "Epoch 144/300\n",
            "7808/7861 [============================>.] - ETA: 0s - loss: 2.2609e-04 - mean_absolute_error: 0.0087\n",
            "Epoch 00144: val_loss did not improve from 0.00004\n",
            "7861/7861 [==============================] - 25s 3ms/sample - loss: 2.2757e-04 - mean_absolute_error: 0.0087 - val_loss: 1.7641e-04 - val_mean_absolute_error: 0.0060\n",
            "Epoch 145/300\n",
            "  64/7861 [..............................] - ETA: 24s - loss: 3.3126e-04 - mean_absolute_error: 0.0090"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMbgVqlC3wlR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " !tensorboard --logdir=\"logs\" --bind_all"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REFE3zIyNXpe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# evaluate the model\n",
        "mse, mae = model.evaluate(data[\"X_test\"], data[\"y_test\"])\n",
        "# calculate the mean absolute error (inverse scaling)\n",
        "mean_absolute_error = data[\"column_scalar\"][\"adjclose\"].inverse_transform(mae.reshape(1,-1))[0][0]\n",
        "print(\"Mean Absolute Error : \", mean_absolute_error)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRBi8GhL9AW_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29qxnMSi_mBb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Predict \n",
        "def predict(model, data, classification=False):\n",
        "  # retrieve the last sequence from data\n",
        "  last_sequence = data[\"last_sequence\"][:N_STEPS]\n",
        "  # retrieve the column scalars\n",
        "  column_scaler = data[\"column_scaler\"]\n",
        "  # reshape the last sequence\n",
        "  last_sequence = last_sequence.reshape((last_sequence.shape[1], last_sequence.shape[0]))\n",
        "  # expand dims\n",
        "  last_sequence = np.expand_dims(last_sequence, axis=0)\n",
        "  # get the prediction (scaled 0 to 1)\n",
        "  prediction = model.predict(last_sequence)\n",
        "  # get the price(by inverting the scaling)\n",
        "  predicted_price = column_scaler[\"adjclose\"].inverse_transform(prediction)[0][0]\n",
        "  return predicted_price"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkdPGQZ5AwO2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# last_seq = data[\"last_sequence\"][:N_STEPS]\n",
        "# column_scaler = data[\"column_scaler\"]\n",
        "# last_seq = last_seq.reshape((last_seq.shape[1], last_seq.shape[0]))\n",
        "# last_seq = np.expand_dims(last_seq, axis=0)\n",
        "# pred = model.predict(last_seq)\n",
        "# print(pred)\n",
        "# print(column_scaler[\"adjclose\"].inverse_transform(pred))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weebBoyEBdzn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# predict the future price\n",
        "future_price = predict(model, data)\n",
        "print(f\"Future price after {LOOKUP_STEP} days is {future_price: .2f}$\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V32lR0N_IXlT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plot the graph\n",
        "def plot_graph(model, data):\n",
        "  y_test = data[\"y_test\"]\n",
        "  X_test = data[\"X_test\"]\n",
        "  y_pred = model.predict(X_test)\n",
        "  y_test = np.squeeze(data[\"column_scaler\"][\"adjclose\"].inverse_transform(np.expand_dims(y_test,axis=0)))\n",
        "  y_pred = np.squeeze(data[\"column_scaler\"][\"adjclose\"].inverse_transform(y_pred))\n",
        "  plt.plot(y_test[-200:], c='b')\n",
        "  plt.plot(y_pred[-200:], c='r')\n",
        "  plt.xlabel(\"Days\")\n",
        "  plt.ylabel(\"Price\")\n",
        "  plt.legend([\"Acutal Price\", \"Predicted Price\"])\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82pK3Io2JeKj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_graph(model, data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypLtXunZJg2l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_accuracy(model, data):\n",
        "    y_test = data[\"y_test\"]\n",
        "    X_test = data[\"X_test\"]\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_test = np.squeeze(data[\"column_scaler\"][\"adjclose\"].inverse_transform(np.expand_dims(y_test, axis=0)))\n",
        "    y_pred = np.squeeze(data[\"column_scaler\"][\"adjclose\"].inverse_transform(y_pred))\n",
        "    y_pred = list(map(lambda current, future: int(float(future) > float(current)), y_test[:-LOOKUP_STEP], y_pred[LOOKUP_STEP:]))\n",
        "    y_test = list(map(lambda current, future: int(float(future) > float(current)), y_test[:-LOOKUP_STEP], y_test[LOOKUP_STEP:]))\n",
        "    return accuracy_score(y_test, y_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Gjs6rFmKabd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(str(LOOKUP_STEP) + \":\", \"Accuracy Score:\", get_accuracy(model, data))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}